	The most glaring point in AI100 Report I found was on page 56, saying "a machine-learning algorithm is only as good as its data set." To an extent, I agree, however, I tend to disagree that is the only aspect affecting machine learning. Throughout this paper, we see several harmful ways that algorithms are passed data, whether it be on Facebook to feature interesting posts to an individual, no matter how racist and bigoted they may be, or minority individuals being denied life-saving care due to a healthcare algorithm. I believe that a machine learning algorithm is only as good as those who design the model as well as its dataset. While I did see ethics boards being encouraged in the paper, not once did I see a suggestion about the inclusion of individuals of minority backgrounds. Whether that be LGBTQ individuals, people of color, or indigenous peoples who can help bridge the gap where the biased (or lack of) data are lacking. 
	According to the New York Times article, Dr.Gerbru of Stanford says “The people creating the technology are a big part of the system. If many are actively excluded from its creation, this technology will benefit a few while harming a great many.” Dr. Gerbu continues to talk about the predominantly white and male industry. She argues that different perspectives help shape AI to avoid racist outcomes. While I believe that the creators have good intentions they lack an essential different point of view. The authors of the AI100 Report also left out the fact of corporate backlash, as Dr.Gerbru experienced when bringing up the issues with Google’s approach to minority issues in AI, perpetuating the problem rather than trying to fix it. The NYT article continues to say “Because the people choosing the training data were mostly white men, they didn’t realize their data was biased.” So yes, an algorithm is only good as its data set. But we can control who creates these training sets to create biased algorithms. 
	Another omitted aspect in this statement is the decades of biased data that has been formed over the years. For example, racist bankers deny a black person a line of credit due to the fact they don’t believe that they can pay it back, without looking at the rest of their financial history. Another example could be sentencing algorithms such as in the criminal justice system, where years of racist outcomes create harsher punishments for colored individuals. The ACLU continues by saying “These algorithms use data such as eviction and criminal histories, which reflect long-standing racial disparities in housing and the criminal legal system that are discriminatory towards marginalized communities.” What AI100 doesn’t address is the long years of system racism that have created these outcomes that are disproportionately weighed against those of people of color. 
	I would also argue that an algorithm is only as good as the purpose for what it is used for. While America has become progressive in its approach to LGBTQ individuals, other countries are steps behind. These individuals have finally started to feel comfortable as a member of this community, and the data on them can either be lacking or used to systematically discriminate against them. Being a member myself, statements like this, scare me as told in VentureBeat. “Prediction algorithms could be deployed at scale by malicious actors, particularly in nations where homosexuality and gender non-conformity are punishable offenses”. AI100 avoids this point altogether and doesn’t make me feel comfortable that the technology could not be used against me in the future. 

	